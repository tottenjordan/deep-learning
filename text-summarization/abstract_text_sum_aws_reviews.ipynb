{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstract-text-sum-aws-reviews.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ugU5fTt2H5Ml"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tottenjordan/deep-learning/blob/master/text-summarization/abstract_text_sum_aws_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uYVoTL5q9I6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook uses a bidirectional sequence-to-sequence RNN to generate text summaries. The text generation method is abstractive, meaning it learns the semantic and contextual meaning between the reveiw and summary and attempts to summarize out-of-sample reveiws with words that have similar semantic and contextual menaing- not just extracting words from that reveiw (extractive method). \n",
        "\n",
        "Jordan Totten"
      ]
    },
    {
      "metadata": {
        "id": "5lhvPaU9gx-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook Setup"
      ]
    },
    {
      "metadata": {
        "id": "bMa69rCegVI7",
        "colab_type": "code",
        "outputId": "68ce0e8d-a7bd-4717-d2de-12f8ced1b0e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zETg_Y9yzoMc",
        "colab_type": "code",
        "outputId": "894f6f04-ccb8-4fe0-fa5c-b1d7786ac95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /content/drive/'My Drive'/'Colab Notebooks'/Amazon-Reviews"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "models\t\t\t  Reviews.csv\t\t      Reviews_pickled.pkl\n",
            "numberbatch-en-17.06.txt  reviews_optimal_sample.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cli1PY4e19gp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Note: I reverted to Tensorflow 1.1.0 because I had trouble with the \"Dynamic Attention\" functions in later TF versions. The Attention methods used in this model have been deprecated in later TF versions. I need to understand the additional parameters before upgrading versions*"
      ]
    },
    {
      "metadata": {
        "id": "En6bcWv5guaa",
        "colab_type": "code",
        "outputId": "8553adf0-6f79-4153-ea7f-6fa8daf5729a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.1.0\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/e4/b2a8bcd1fa689489050386ec70c5c547e4a75d06f2cc2b55f45463cd092c/tensorflow-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.4MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1.0) (0.33.1)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1.0) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1.0) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1.0) (1.14.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1.0) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1.0) (40.8.0)\n",
            "\u001b[31mstable-baselines 2.2.1 has requirement tensorflow>=1.5.0, but you'll have tensorflow 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mmagenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed tensorflow-1.1.0\n",
            "TensorFlow Version: 1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6nPKadovg1t3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reviews Data\n",
        "This dataset comes from the [Amazon Product Review Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews). The original dataset has +500,000 reviews. The reveiws selected for this study were preprocessed locally. I created a \"usefulness\" index which is the number of users who found the review helpful divided by the number of users who indicated whether or not the review was helpful. The reviews with the top 25,001 usefulness index score were used.\n",
        "\n",
        "\n",
        "\n",
        "1.  Product ID\n",
        "2.  User ID\n",
        "3. HelpfulnessNumerator is the number of users who found the reveiw helpful\n",
        "4. HelpfulnessDeominator is the number of users who indicate whether they found the review helpful or not\n",
        "5. Score is a rating between 1 and 5 and references the product being reviewed\n",
        "6. Time is a timestamp\n",
        "7. Summary is a brief summary of the review\n",
        "8. Text of the review\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9MBIlaQXg5cl",
        "colab_type": "code",
        "outputId": "ab0591c3-826f-4dba-ddc4-e64ced50be91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/reviews_optimal_sample.csv\")\n",
        "reviews.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "nN6mPdhX3OlG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Sanity Checks"
      ]
    },
    {
      "metadata": {
        "id": "-ie9Na5VhrDG",
        "colab_type": "code",
        "outputId": "0e001854-8a89-4ee4-a35f-1b1ff0efd2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "reviews.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>usefulness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64422</td>\n",
              "      <td>B000MIDROQ</td>\n",
              "      <td>A161DK06JJMCYF</td>\n",
              "      <td>J. E. Stephens \"Jeanne\"</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1224892800</td>\n",
              "      <td>Bought This for My Son at College</td>\n",
              "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44737</td>\n",
              "      <td>B001EQ55RW</td>\n",
              "      <td>A2V0I904FH7ABY</td>\n",
              "      <td>Ram</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1212883200</td>\n",
              "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
              "      <td>It was almost a 'love at first bite' - the per...</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>508796</td>\n",
              "      <td>B001EO7GFS</td>\n",
              "      <td>A1CRI3DKT18JBX</td>\n",
              "      <td>D. Burck</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1251331200</td>\n",
              "      <td>excellent value</td>\n",
              "      <td>Excellent value for a premium product. As typi...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>386062</td>\n",
              "      <td>B004AGBYA0</td>\n",
              "      <td>AA8OP79Z9GMQL</td>\n",
              "      <td>Tudor</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1333065600</td>\n",
              "      <td>Another good product bites the dust?</td>\n",
              "      <td>This was my favourite brand of cod liver, it's...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8157</td>\n",
              "      <td>B001IZA8S0</td>\n",
              "      <td>A1KKE6VX8VPWZK</td>\n",
              "      <td>J. J. Marino \"Geekasaurus Rex\"</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1278201600</td>\n",
              "      <td>Great tasting tea.</td>\n",
              "      <td>We usually have fresh lemon grass tea at our l...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Id   ProductId          UserId                     ProfileName  \\\n",
              "0   64422  B000MIDROQ  A161DK06JJMCYF         J. E. Stephens \"Jeanne\"   \n",
              "1   44737  B001EQ55RW  A2V0I904FH7ABY                             Ram   \n",
              "2  508796  B001EO7GFS  A1CRI3DKT18JBX                        D. Burck   \n",
              "3  386062  B004AGBYA0   AA8OP79Z9GMQL                           Tudor   \n",
              "4    8157  B001IZA8S0  A1KKE6VX8VPWZK  J. J. Marino \"Geekasaurus Rex\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                   3.0                       1      5  1224892800   \n",
              "1                   3.0                       2      4  1212883200   \n",
              "2                   5.0                       5      4  1251331200   \n",
              "3                   1.0                       1      1  1333065600   \n",
              "4                   8.0                       8      5  1278201600   \n",
              "\n",
              "                                        Summary  \\\n",
              "0             Bought This for My Son at College   \n",
              "1  Pure cocoa taste with crunchy almonds inside   \n",
              "2                               excellent value   \n",
              "3          Another good product bites the dust?   \n",
              "4                            Great tasting tea.   \n",
              "\n",
              "                                                Text  usefulness  \n",
              "0  My son loves spaghetti so I didn't hesitate or...         3.0  \n",
              "1  It was almost a 'love at first bite' - the per...         1.5  \n",
              "2  Excellent value for a premium product. As typi...         1.0  \n",
              "3  This was my favourite brand of cod liver, it's...         1.0  \n",
              "4  We usually have fresh lemon grass tea at our l...         1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "lfUcnMgaw_eV",
        "colab_type": "code",
        "outputId": "66677a6c-2dd9-469d-eb3b-a1917e8f7a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# Check for any nulls values\n",
        "reviews.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                        0\n",
              "ProductId                 0\n",
              "UserId                    0\n",
              "ProfileName               0\n",
              "HelpfulnessNumerator      0\n",
              "HelpfulnessDenominator    0\n",
              "Score                     0\n",
              "Time                      0\n",
              "Summary                   0\n",
              "Text                      0\n",
              "usefulness                0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "RBQOUD8wxCc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove null values and unneeded features\n",
        "# reviews = reviews.dropna()\n",
        "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n",
        "                        'Score','Time','usefulness'], 1)\n",
        "reviews = reviews.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CkvDbO2SxK8S",
        "colab_type": "code",
        "outputId": "47bffdcb-318b-4e50-9847-8cc9d4415c30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "reviews.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bought This for My Son at College</td>\n",
              "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
              "      <td>It was almost a 'love at first bite' - the per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>excellent value</td>\n",
              "      <td>Excellent value for a premium product. As typi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Another good product bites the dust?</td>\n",
              "      <td>This was my favourite brand of cod liver, it's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great tasting tea.</td>\n",
              "      <td>We usually have fresh lemon grass tea at our l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Summary  \\\n",
              "0             Bought This for My Son at College   \n",
              "1  Pure cocoa taste with crunchy almonds inside   \n",
              "2                               excellent value   \n",
              "3          Another good product bites the dust?   \n",
              "4                            Great tasting tea.   \n",
              "\n",
              "                                                Text  \n",
              "0  My son loves spaghetti so I didn't hesitate or...  \n",
              "1  It was almost a 'love at first bite' - the per...  \n",
              "2  Excellent value for a premium product. As typi...  \n",
              "3  This was my favourite brand of cod liver, it's...  \n",
              "4  We usually have fresh lemon grass tea at our l...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "fN4MFjhyx-8G",
        "colab_type": "code",
        "outputId": "50e51e5f-7a40-4bbd-881c-71c736cb0947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspecting some of the reviews\n",
        "for i in range(5):\n",
        "    print(\"Review #\",i+1)\n",
        "    print(reviews.Summary[i])\n",
        "    print(reviews.Text[i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review # 1\n",
            "Bought This for My Son at College\n",
            "My son loves spaghetti so I didn't hesitate ordering this for him. He says they are great. I have tried them myself and they are delicious. Just open and pop them in the microwave. It is very easy. The best thing about ordering from Amazon grocery is that they deliver to your door. If you have a loved one that lives far away and may have limited transportation this is the answer. Just order what you want them to have and Amazon takes care of the rest.\n",
            "\n",
            "Review # 2\n",
            "Pure cocoa taste with crunchy almonds inside\n",
            "It was almost a 'love at first bite' - the perfectly roasted almond with a nice thin layer of pure flavorful cocoa on the top.<br /><br />You can smell the cocoa as soon as you open the canister - making you want to take a bite.<br /><br />You may or may not like the taste of this cocoa roasted almonds depending on your likingness for cocoa.  We are so much used to the taste of chocolate (which is actually cocoa + many other ingredients like milk ...) - that you might have never really tasted really cocoa.<br /><br />Tasting this item it like tasting and enjoying flavorful pure raw cocoa with crunchy almonds in the center.  Get yourself a box and see for yourself what real cocoa + almonds is !<br /><br />Where this product loses a star is in its packaging - the external sleeve is kind of comes in one piece, so if you try to remove the lid, the external sleeve kind of tends to come off fully - so careful when you are removing the external sleeve for the canister.\n",
            "\n",
            "Review # 3\n",
            "excellent value\n",
            "Excellent value for a premium product. As typical for dried product, as many broken pieces as whole mushrooms, but immaterial for something usually diced after reconstituting. Great flavor boost for soups, sauces, risottos - really anything braised, simmered or blended.\n",
            "\n",
            "Review # 4\n",
            "Another good product bites the dust?\n",
            "This was my favourite brand of cod liver, it's hard to find in Toronto, so I buy a dozen at a time when I find it. However, the last purchase, every one of the cans I opened was very lightly packed, about one third liver and the rest just oil. Even worse, the liver had dark, grey spots that would turn my stomach just looking at them. Perhaps good cod liver has become a thing of the past, like many other delicacies. I would prefer if they just raised the price to whatever it needs to be to provide the quality that we deserve when we spend our hard earned money, or, if that's not possible, just let the fish live and make soyburgers or something.\n",
            "\n",
            "Review # 5\n",
            "Great tasting tea.\n",
            "We usually have fresh lemon grass tea at our local Zen center. It is extremely easy to make, put a few blades of lemon grass in a pot to steep and you will get a fantastic tasting tea.<br /><br />This tea is here is a fine replacement for when you cannot get fresh lemon grass. Lemon grass has many health benefits. It is said to detoxify the liver and can lower uric acid levels, this is important if you have gout.<br /><br />Because you get such a large quantity here on Amazon it should last you quite a while. Caffeine free and light it makes a very refreshing afternoon tea.<br /><br />Give it a try!<br /><br />Thank you for reading my review.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PgUzkEog3eyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Natural Language Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "HRh9Xeo3Lol8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Remove unwanted characters and contractions from reviews and summaries\n",
        "* Remove stopwords from reviews training data, but leave stopwords in summaries (labels)\n",
        "* Stopwords in summaries will enable more natural review summaries in the generative model"
      ]
    },
    {
      "metadata": {
        "id": "TDstALEyIv94",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}\n",
        "\n",
        "\n",
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WyzycsBnJMiL",
        "colab_type": "code",
        "outputId": "6f0525df-e155-4d39-f8a4-e3e7519a3e31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Keep stopwords for summaries \n",
        "# Clean the summaries and texts\n",
        "clean_summaries = []\n",
        "for summary in reviews.Summary:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in reviews.Text:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uFkjXy4mMUpJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sanity check on cleaned summaries"
      ]
    },
    {
      "metadata": {
        "id": "VYAiZWZ3J17z",
        "colab_type": "code",
        "outputId": "b6e96499-1e07-4a31-ff18-d85885d78e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
        "for i in range(5):\n",
        "    print(\"Clean Review #\",i+1)\n",
        "    print(clean_summaries[i])\n",
        "    print(clean_texts[i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean Review # 1\n",
            "bought this for my son at college\n",
            "son loves spaghetti hesitate ordering says great tried delicious open pop microwave easy best thing ordering amazon grocery deliver door loved one lives far away may limited transportation answer order want amazon takes care rest\n",
            "\n",
            "Clean Review # 2\n",
            "pure cocoa taste with crunchy almonds inside\n",
            "almost love first bite perfectly roasted almond nice thin layer pure flavorful cocoa top <br ><br >you smell cocoa soon open canister making want take bite <br ><br >you may may like taste cocoa roasted almonds depending likingness cocoa much used taste chocolate actually cocoa many ingredients like milk might never really tasted really cocoa <br ><br >tasting item like tasting enjoying flavorful pure raw cocoa crunchy almonds center get box see real cocoa almonds <br ><br >where product loses star packaging external sleeve kind comes one piece try remove lid external sleeve kind tends come fully careful removing external sleeve canister\n",
            "\n",
            "Clean Review # 3\n",
            "excellent value\n",
            "excellent value premium product typical dried product many broken pieces whole mushrooms immaterial something usually diced reconstituting great flavor boost soups sauces risottos really anything braised simmered blended\n",
            "\n",
            "Clean Review # 4\n",
            "another good product bites the dust \n",
            "favourite brand cod liver hard find toronto buy dozen time find however last purchase every one cans opened lightly packed one third liver rest oil even worse liver dark grey spots would turn stomach looking perhaps good cod liver become thing past like many delicacies would prefer raised price whatever needs provide quality deserve spend hard earned money possible let fish live make soyburgers something\n",
            "\n",
            "Clean Review # 5\n",
            "great tasting tea \n",
            "usually fresh lemon grass tea local zen center extremely easy make put blades lemon grass pot steep get fantastic tasting tea <br ><br >this tea fine replacement cannot get fresh lemon grass lemon grass many health benefits said detoxify liver lower uric acid levels important gout <br ><br >because get large quantity amazon last quite caffeine free light makes refreshing afternoon tea <br ><br >give try <br ><br >thank reading review\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ut6zqDWnJ65C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function to count vocabulary\n",
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qoojNcgJ98E",
        "colab_type": "code",
        "outputId": "e2d22a97-b994-4527-fc8c-39ec2423853d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Find the number of times each word was used and the size of the vocabulary\n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 34217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PVtoHqlT36ew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings Matrix\n",
        "\n",
        "Conceptnet Numberbatch's (CN) embeddings are used. CN is an ensemble of many pre-trained vectors (including GloVe)\n",
        "(https://github.com/commonsense/conceptnet-numberbatch)\n"
      ]
    },
    {
      "metadata": {
        "id": "FVezoEpuKDqX",
        "colab_type": "code",
        "outputId": "bd8262ab-441a-4fc8-fda5-a7621ebd1352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load Conceptnet Numberbatch's (CN) embeddings \n",
        "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/numberbatch-en-17.06.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 417195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2zHSedgRO4si",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Find the number of words in our reviews corpus that are missing from the CN embedding\n",
        "* Set a frequency threshold of 20 (i.e., words occuring less than 20 times in the reviews that are not in the CN embedding will not be identified)\n",
        "* A threshold ensures that the missing words from the CN embedding are represented in our word embedding matrix "
      ]
    },
    {
      "metadata": {
        "id": "Wzb7q0hYPgmG",
        "colab_type": "code",
        "outputId": "b946d4e4-4e81-415a-9df3-414824549d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Find the number of words that are missing from CN, \n",
        "# and are used greater than or equal to our threshold.\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 258\n",
            "Percent of words that are missing from vocabulary: 0.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dWyOedev6FQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* UNK is an unknown word\n",
        "* GO represents the first token fed to a decoder \n",
        "* EOS indicates the token that represents the \"end of setence.\" A decoder recognizes this as the end of an answer (punctuation not used!)\n",
        "* PAD ensures each sequence in a training batch is the same length \n"
      ]
    },
    {
      "metadata": {
        "id": "cHcu0jABQMk9",
        "colab_type": "code",
        "outputId": "226cef57-9bb2-4f53-a8ca-3de4c20fb196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in CN\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 34217\n",
            "Number of words we will use: 23912\n",
            "Percent of words we will use: 69.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B3I_NtiPRC8b",
        "colab_type": "code",
        "outputId": "866db5ed-2d7f-48a5-b84c-5add26f2072d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xCR3yTmqBnIj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Convert words in text to an integer \n",
        "* If word is not in vocab_to_int, use UNK's integer\n",
        "* Total the number of words and UNKs\n",
        "* Add EOS token to the end of the texts\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eig4otqpRSwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iwRlRI1LRVfJ",
        "colab_type": "code",
        "outputId": "16f6d152-285e-4496-c454-e4a011012d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 1218950\n",
            "Total number of UNKs in headlines: 20945\n",
            "Percent of words that are UNK: 1.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZQZLuMTSRaty",
        "colab_type": "code",
        "outputId": "39f97579-6035-4630-bfa0-5fada5b6c69b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])\n",
        "  \n",
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "             counts\n",
            "count  25000.000000\n",
            "mean       4.250480\n",
            "std        2.654133\n",
            "min        0.000000\n",
            "25%        2.000000\n",
            "50%        4.000000\n",
            "75%        6.000000\n",
            "max       24.000000\n",
            "\n",
            "Texts:\n",
            "             counts\n",
            "count  25000.000000\n",
            "mean      45.507520\n",
            "std       42.587654\n",
            "min        1.000000\n",
            "25%       20.000000\n",
            "50%       33.000000\n",
            "75%       56.000000\n",
            "max      996.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GpKDxcCXRjAg",
        "colab_type": "code",
        "outputId": "866df72c-0014-488d-fccf-89d600d8058f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspect the length of texts\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.0\n",
            "121.0\n",
            "215.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NPjtnrlHRoZG",
        "colab_type": "code",
        "outputId": "4f264b15-c17a-478f-e187-05c910c0c755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.0\n",
            "9.0\n",
            "13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dLx19aqtRq2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGXxXJKSRveX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Sort the summaries and tests by the length of the texts, shortest to longest\n",
        "* Limit the length of summaries and texts based on the min and max ranges\n",
        "* Remove reviews that include too many UNKs"
      ]
    },
    {
      "metadata": {
        "id": "KbGXvLsqR8CB",
        "colab_type": "code",
        "outputId": "3fcab96e-4de9-4c27-ec0e-b77af4bde9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 84\n",
        "max_summary_length = 13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16827\n",
            "16827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nMDGnYdqSCir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the Model"
      ]
    },
    {
      "metadata": {
        "id": "cfP2bP7rScgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for creating network"
      ]
    },
    {
      "metadata": {
        "id": "R5faH32RSE7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUvYhCBBSheA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for preparing the input data for encoding\n",
        "* remove the last word ID from each batch and concatenate the <\"GO\"> to the begining of each batch"
      ]
    },
    {
      "metadata": {
        "id": "lGAQyP6tSVkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "       \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4Aie9hRSsTV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for building encoding layers"
      ]
    },
    {
      "metadata": {
        "id": "SFLgE2hXSZ1K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "        \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zfdcPowSSw2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for creating the training logits"
      ]
    },
    {
      "metadata": {
        "id": "32WIpGFaSzcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "        \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mawqK_WGTltk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for creating the inference logits"
      ]
    },
    {
      "metadata": {
        "id": "rwrFdinqTjmO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "        \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7gVlC07Tukj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for creating the decoding layers\n",
        "* attention created for training and inference layers\n",
        "\n",
        "Attention source: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb "
      ]
    },
    {
      "metadata": {
        "id": "CHg4ci3uTsLn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "        \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) \n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
        "            \n",
        "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],_zero_state_tensors(rnn_size, batch_size, tf.float32))\n",
        "                                                      \n",
        "    \n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R48fApd8VcMP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create Sequence to Sequence Model\n",
        "* uses previous functions to create the training and inference logits"
      ]
    },
    {
      "metadata": {
        "id": "dHLfUnxZVZjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "        \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ry5zFvb-Vpxe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* PAD sentences so that each sequence of a batch has the same length\n",
        "* Define batch summaries, reviews, and the length of their sentences together"
      ]
    },
    {
      "metadata": {
        "id": "Vd0VyaJeVwOM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "  \n",
        "def get_batches(summaries, texts, batch_size):\n",
        "        \n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TC4zLRnmWKDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set hyperparameters for model"
      ]
    },
    {
      "metadata": {
        "id": "k9lUqHx_WMZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.75 # 0.50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sy6urPKhWPGF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build graph"
      ]
    },
    {
      "metadata": {
        "id": "vnsBW-rpWU4H",
        "colab_type": "code",
        "outputId": "ca26d178-484c-46db-bb7d-446a0defc30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D1Z4SU2Kp4bQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ]
    },
    {
      "metadata": {
        "id": "3-G6G9uMqpCs",
        "colab_type": "code",
        "outputId": "876b5b9a-f388-4a89-bbf2-2220d3cbf99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import *\n",
        "import os\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "PATH = os.getcwd()\n",
        "import gensim\n",
        "#LOG_DIR = PATH + 'word_embed-logging'\n",
        "#metadata = os.path.join(LOG_DIR, 'metadata.tsv')\n",
        "len(sorted_summaries)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16827"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "QiW_fk3Dp60u",
        "colab_type": "code",
        "outputId": "c46fb757-1fd4-435a-bef6-3d9e19074b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Subset the data for training\n",
        "start = 0\n",
        "end = start + 10000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 2\n",
            "The longest text length: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvDdzgucrPeN",
        "colab_type": "code",
        "outputId": "00ef1a57-f182-4044-9d2b-ee42d276a54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 3 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "# Create a summary operation to log the progress of the network\n",
        "with tf.variable_scope('logging_m22'):\n",
        "    tf.summary.scalar('current_cost', cost)\n",
        "    summary = tf.summary.merge_all()\n",
        "\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
        "    #loader.restore(sess, checkpoint)\n",
        "    training_writer = tf.summary.FileWriter('./logs/training/m1', sess.graph)\n",
        "    \n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, path)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e54c214f2df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Create a summary operation to log the progress of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logging_m22'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current_cost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cost' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ugU5fTt2H5Ml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save Model in Colab"
      ]
    },
    {
      "metadata": {
        "id": "pUI0-l1y8XYw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()                       \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B1WuPfav8h13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('seq2seq-text-gen-train.h5')\n",
        "model_file = drive.CreateFile({'Abs-Text-Summarization': 'seq2seq-text-gen-train.h5'})\n",
        "model_file.SetContentFile('seq2seq-text-gen-train.h5')\n",
        "model_file.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gkNp7AnPUGo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text Generation\n",
        "* Create a review or use a review from the dataset\n",
        "* Summary length is set to random"
      ]
    },
    {
      "metadata": {
        "id": "scS0YGjNOmhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghx-wM4HRDMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here I grab random words from the reveiws to create a test reveiw. "
      ]
    },
    {
      "metadata": {
        "id": "AfaSS2C8RvHG",
        "colab_type": "code",
        "outputId": "7935b8bb-34dc-4759-84ba-f23a03f63f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a reveiw in 'input_sentence' \n",
        "# or generate a random review from training data\n",
        "\n",
        "#input_sentence = \"I can make up my own review for testing\"\n",
        "#text = text_to_seq(input_sentence)\n",
        "random = np.random.randint(0,len(clean_texts))\n",
        "input_sentence = clean_texts[random]\n",
        "text = text_to_seq(clean_texts[random])\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: coffe average slight hint pumpkin spice want overwhelmingly flavored coffee <br ><br >works great office would buy\n",
            "\n",
            "Text\n",
            "  Word Ids:    [1187, 118, 3333, 1619, 752, 305, 888, 7371, 222, 134, 6903, 6904, 23908, 22, 3204, 103, 161]\n",
            "  Input Words: coffe average slight hint pumpkin spice want overwhelmingly flavored coffee <br ><br <UNK> great office would buy\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [188, 97, 427, 735]\n",
            "  Response Words: too much decaf mocha\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAjEpa5cPhPl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is an actual reveiw from the training data. \n",
        "\n",
        "Original Summary: \"bought this for my son at college\""
      ]
    },
    {
      "metadata": {
        "id": "l8HmkKPvnyGF",
        "colab_type": "code",
        "outputId": "07d7b362-5e4c-48df-b686-76c5c4e106dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"My son loves spaghetti so I didn't hesitate ordering this for him.\\\n",
        "  He says they are great. I have tried them myself and they are delicious. Just open \\\n",
        "  and pop them in the microwave. It is very easy. The best thing about ordering from Amazon grocery is that they deliver to your door. \\\n",
        "  If you have a loved one that lives far away and may have limited transportation this is the answer. Just order what you want them to have and Amazon takes care of the rest.\"\n",
        "\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: My son loves spaghetti so I didn't hesitate ordering this for him.  He says they are great. I have tried them myself and they are delicious. Just open   and pop them in the microwave. It is very easy. The best thing about ordering from Amazon grocery is that they deliver to your door.   If you have a loved one that lives far away and may have limited transportation this is the answer. Just order what you want them to have and Amazon takes care of the rest.\n",
            "\n",
            "Text\n",
            "  Word Ids:    [4, 148, 3463, 6901, 2977, 564, 22, 731, 83, 348, 1107, 2737, 396, 48, 77, 2977, 921, 1618, 4094, 848, 331, 265, 3328, 315, 559, 1277, 1869, 6902, 5378, 875, 888, 921, 3529, 264, 1244]\n",
            "  Input Words: son loves spaghetti hesitate ordering says great tried delicious open pop microwave easy best thing ordering amazon grocery deliver door loved one lives far away may limited transportation answer order want amazon takes care rest\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [215, 55]\n",
            "  Response Words: love it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ItkOMeznRpBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here I create my own review. As seen, the model does not correct summarize the intent of the review. Perhaps it is being sarcastic!"
      ]
    },
    {
      "metadata": {
        "id": "xx9tCmTpoarR",
        "colab_type": "code",
        "outputId": "becede99-1852-44e5-f7b3-6ed230b9798c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"Disgusting cheese. I will never order this again\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: Disgusting cheese. I will never order this again\n",
            "\n",
            "Text\n",
            "  Word Ids:    [1410, 689, 779, 875]\n",
            "  Input Words: disgusting cheese never order\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [2446, 466]\n",
            "  Response Words: utterly fantastic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cGcUvz68o8WL",
        "colab_type": "code",
        "outputId": "8bb267c2-c91b-4282-b537-b686b41641c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"Terrible book. I wish to return it\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: Terrible book. I wish to return it\n",
            "\n",
            "Text\n",
            "  Word Ids:    [583, 3115, 491, 484]\n",
            "  Input Words: terrible book wish return\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [281, 281, 1927, 55, 583]\n",
            "  Response Words: cannot cannot stop it terrible\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NoVp_8LLp4tc",
        "colab_type": "code",
        "outputId": "be841771-c3d2-4993-9391-0458255cf5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"almost love first bite perfectly roasted almond nice thin layer pure flavorful cocoa\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: almost love first bite perfectly roasted almond nice thin layer pure flavorful cocoa\n",
            "\n",
            "Text\n",
            "  Word Ids:    [425, 215, 861, 1231, 436, 2082, 587, 78, 4168, 6239, 7, 404, 8]\n",
            "  Input Words: almost love first bite perfectly roasted almond nice thin layer pure flavorful cocoa\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [516, 516]\n",
            "  Response Words: yum yum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n419Ly-_T5sO",
        "colab_type": "code",
        "outputId": "93571bf8-c0c6-4f04-a2c3-70baa5445ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"It was almost a 'love at first bite' - the perfectly roasted \\\n",
        "almond with a nice thin layer of pure flavorful cocoa on the top.<br /><br />You\\\n",
        "can smell the cocoa as soon as you open the canister - making you want to take a \\\n",
        "bite.<br /><br />You may or may not like the taste of this cocoa roasted almonds\\\n",
        "depending on your likingness for cocoa.  We are so much used to the taste of \\\n",
        "chocolate (which is actually cocoa + many other ingredients like milk ...) - that \\\n",
        "you might have never really tasted really cocoa.<br /><br />Tasting this item it \\\n",
        "like tasting and enjoying flavorful pure raw cocoa with crunchy almonds in the \\\n",
        "center.  Get yourself a box and see for yourself what real cocoa + almonds \\\n",
        "is !<br /><br />Where this product \"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: It was almost a 'love at first bite' - the perfectly roasted almond with a nice thin layer of pure flavorful cocoa on the top.<br /><br />You can smell the cocoa as soon as you open the canister - making you want to take a bite.<br /><br />You may or may not like the taste of this cocoa roasted almonds depending on your likingness for cocoa.  We are so much used to the taste of chocolate (which is actually cocoa + many other ingredients like milk ...) - that you might have never really tasted really cocoa.<br /><br />Tasting this item it like tasting and enjoying flavorful pure raw cocoa with crunchy almonds in the center.  Get yourself a box and see for yourself what real cocoa + almonds is !<br /><br />Where this product \n",
            "\n",
            "Text\n",
            "  Word Ids:    [425, 215, 861, 1231, 436, 2082, 587, 78, 4168, 6239, 7, 404, 8, 361, 6903, 6904, 6905, 962, 8, 685, 348, 4092, 972, 888, 772, 1231, 6903, 6904, 6905, 1277, 1277, 46, 9, 8, 2082, 12, 2999, 23908, 8, 97, 368, 9, 30, 2160, 8, 469, 1505, 46, 120, 2444, 779, 286, 317, 286, 8, 6903, 6904, 23908, 353, 46, 23, 2560, 404, 7, 1628, 8, 11, 12, 4022, 42, 759, 1286, 217, 8, 12, 6903, 6904, 23908, 18]\n",
            "  Input Words: almost love first bite perfectly roasted almond nice thin layer pure flavorful cocoa top <br ><br >you smell cocoa soon open canister making want take bite <br ><br >you may may like taste cocoa roasted almonds depending <UNK> cocoa much used taste chocolate actually cocoa many ingredients like milk might never really tasted really cocoa <br ><br <UNK> item like tasting enjoying flavorful pure raw cocoa crunchy almonds center get box see real cocoa almonds <br ><br <UNK> product\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [2300, 403, 5149, 23, 138]\n",
            "  Response Words: totally amazing suited tasting cookie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZlPlr3YdT5bY",
        "colab_type": "code",
        "outputId": "12c9a3b1-66d7-4502-f59a-1d39fff188d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"Excellent value for a premium product. As typical for dried product, \\\n",
        "as many broken pieces as whole mushrooms, but immaterial for something usually diced after \\\n",
        "reconstituting. Great flavor boost for soups, sauces, risottos - really anything braised, simmered or blended.\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: Excellent value for a premium product. As typical for dried product, as many broken pieces as whole mushrooms, but immaterial for something usually diced after reconstituting. Great flavor boost for soups, sauces, risottos - really anything braised, simmered or blended.\n",
            "\n",
            "Text\n",
            "  Word Ids:    [14, 15, 696, 18, 2587, 608, 18, 469, 1169, 1214, 1560, 1045, 6909, 835, 3997, 4776, 6910, 22, 163, 3221, 673, 2600, 23908, 286, 1273, 6911, 6912, 4411]\n",
            "  Input Words: excellent value premium product typical dried product many broken pieces whole mushrooms immaterial something usually diced reconstituting great flavor boost soups sauces <UNK> really anything braised simmered blended\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [14, 15]\n",
            "  Response Words: excellent value\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zwXZHm1vT5Le",
        "colab_type": "code",
        "outputId": "d1b08891-8171-452d-9330-d80c308c6c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"This was my favourite brand of cod liver, it's hard to find in Toronto, \\\n",
        "so I buy a dozen at a time when I find it. However, the last purchase, every one of the cans \\\n",
        "I opened was very lightly packed, about one third liver and the rest just oil. Even worse, the liver had \\\n",
        "dark, grey spots that would turn my stomach just looking at them. Perhaps good cod liver has become a \\\n",
        "thing of the past, like many other delicacies. I would prefer if they just raised the price to whatever \\\n",
        "it needs to be to provide the quality that we deserve when we spend our hard earned money, or, if that's \\\n",
        "not possible, just let the fish live and make soyburgers or something.\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: This was my favourite brand of cod liver, it's hard to find in Toronto, so I buy a dozen at a time when I find it. However, the last purchase, every one of the cans I opened was very lightly packed, about one third liver and the rest just oil. Even worse, the liver had dark, grey spots that would turn my stomach just looking at them. Perhaps good cod liver has become a thing of the past, like many other delicacies. I would prefer if they just raised the price to whatever it needs to be to provide the quality that we deserve when we spend our hard earned money, or, if that's not possible, just let the fish live and make soyburgers or something.\n",
            "\n",
            "Text\n",
            "  Word Ids:    [3289, 370, 6913, 939, 347, 494, 6914, 161, 6915, 122, 494, 2292, 1230, 535, 949, 265, 237, 6365, 1119, 2563, 265, 5161, 939, 1244, 498, 99, 1779, 939, 201, 147, 251, 103, 3271, 395, 2309, 2610, 17, 6913, 939, 196, 77, 742, 46, 469, 6916, 103, 1625, 2073, 135, 385, 1196, 447, 253, 1857, 1323, 347, 6917, 1186, 6918, 1836, 1912, 1583, 397, 23908, 835]\n",
            "  Input Words: favourite brand cod liver hard find toronto buy dozen time find however last purchase every one cans opened lightly packed one third liver rest oil even worse liver dark grey spots would turn stomach looking perhaps good cod liver become thing past like many delicacies would prefer raised price whatever needs provide quality deserve spend hard earned money possible let fish live make <UNK> something\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [425, 79, 17, 637]\n",
            "  Response Words: almost of good color\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JnQmhZS-T4_5",
        "colab_type": "code",
        "outputId": "c6200ff5-99fd-41ff-8586-5bf982d362e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"We usually have fresh lemon grass tea at our local Zen center. \\\n",
        "It is extremely easy to make, put a few blades of lemon grass in a pot to steep \\\n",
        "and you will get a fantastic tasting tea.<br /><br />This tea is here is a fine \\\n",
        "replacement for when you cannot get fresh lemon grass. Lemon grass has many health \\\n",
        "benefits. It is said to detoxify the liver and can lower uric acid levels, this is \\\n",
        "important if you have gout.<br /><br />Because you get such a large quantity here on \\\n",
        "Amazon it should last you quite a while. Caffeine free and light it makes a very \\\n",
        "refreshing afternoon tea.<br /><br />Give it a try!<br /><br />Thank you for reading my review.\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "\n",
        "#checkpoint = \"./best_model.ckpt\"\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/{checkpoint}\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(path + '.meta')\n",
        "    loader.restore(sess, path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding \n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', input_sentence)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/Amazon-Reviews/models/best_model.ckpt\n",
            "Original Text: We usually have fresh lemon grass tea at our local Zen center. It is extremely easy to make, put a few blades of lemon grass in a pot to steep and you will get a fantastic tasting tea.<br /><br />This tea is here is a fine replacement for when you cannot get fresh lemon grass. Lemon grass has many health benefits. It is said to detoxify the liver and can lower uric acid levels, this is important if you have gout.<br /><br />Because you get such a large quantity here on Amazon it should last you quite a while. Caffeine free and light it makes a very refreshing afternoon tea.<br /><br />Give it a try!<br /><br />Thank you for reading my review.\n",
            "\n",
            "Text\n",
            "  Word Ids:    [3997, 551, 130, 2637, 24, 2353, 3855, 4022, 1184, 396, 397, 768, 6919, 130, 2637, 700, 4529, 42, 466, 23, 24, 6903, 6904, 6920, 24, 763, 1123, 281, 42, 551, 130, 2637, 130, 2637, 469, 1268, 1487, 1630, 6921, 939, 1234, 6922, 967, 5463, 6923, 3561, 6903, 6904, 23908, 42, 1321, 2698, 921, 1230, 536, 446, 63, 434, 246, 180, 648, 24, 6903, 6904, 23908, 296, 6903, 6904, 6924, 5650, 497]\n",
            "  Input Words: usually fresh lemon grass tea local zen center extremely easy make put blades lemon grass pot steep get fantastic tasting tea <br ><br >this tea fine replacement cannot get fresh lemon grass lemon grass many health benefits said detoxify liver lower uric acid levels important gout <br ><br <UNK> get large quantity amazon last quite caffeine free light makes refreshing afternoon tea <br ><br <UNK> try <br ><br >thank reading review\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [17, 637, 163]\n",
            "  Response Words: good color flavor\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}